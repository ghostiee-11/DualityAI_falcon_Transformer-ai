{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12615309,"sourceType":"datasetVersion","datasetId":7969646}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T20:34:39.187618Z","iopub.execute_input":"2025-08-01T20:34:39.187844Z","iopub.status.idle":"2025-08-01T20:34:40.905063Z","shell.execute_reply.started":"2025-08-01T20:34:39.187820Z","shell.execute_reply":"2025-08-01T20:34:40.904409Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\n\n# --- 1. Define Your Paths ---\n# Source directory (read-only)\nsource_dir = '/kaggle/input/falcon/HackByte_Dataset'\n\n# Destination directory (writable)\nworking_dir = '/kaggle/working/'\n\n# --- 2. List the files you need to copy ---\nfiles_to_copy = ['yolo_params.yaml', 'train.py', 'predict.py']\n\n# --- 3. Copy each file ---\nfor file_name in files_to_copy:\n    source_file = os.path.join(source_dir, file_name)\n    destination_file = os.path.join(working_dir, file_name)\n    \n    # Copy the file\n    shutil.copy(source_file, destination_file)\n    print(f\"Copied '{file_name}' to {working_dir}\")\n\nprint(\"\\nFiles are now in your writable directory!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T08:40:20.458215Z","iopub.execute_input":"2025-08-02T08:40:20.458873Z","iopub.status.idle":"2025-08-02T08:40:20.470335Z","shell.execute_reply.started":"2025-08-02T08:40:20.458847Z","shell.execute_reply":"2025-08-02T08:40:20.469563Z"}},"outputs":[{"name":"stdout","text":"Copied 'yolo_params.yaml' to /kaggle/working/\nCopied 'train.py' to /kaggle/working/\nCopied 'predict.py' to /kaggle/working/\n\nFiles are now in your writable directory!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Install the library to handle YAML files\n!pip install pyyaml\n\nimport yaml\n\n# Path to the COPIED yaml file\nyaml_file_path = '/kaggle/working/yolo_params.yaml'\n\n# --- 1. Load the yaml file ---\nwith open(yaml_file_path, 'r') as file:\n    yolo_config = yaml.safe_load(file)\n\n# --- 2. Make your edits ---\n# For example, let's say you want to change the path to be sure it's correct\nyolo_config['path'] = '/kaggle/input/falcon/HackByte_Dataset/data' \n\n# Or maybe you want to add a new parameter (this part is for later experiments)\n# yolo_config['epochs'] = 100 \n\n# --- 3. Save the changes back to the file ---\nwith open(yaml_file_path, 'w') as file:\n    yaml.dump(yolo_config, file)\n\nprint(\"yolo_params.yaml has been updated successfully!\")\n# You can print the config to verify\nprint(yolo_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T08:40:27.020126Z","iopub.execute_input":"2025-08-02T08:40:27.020381Z","iopub.status.idle":"2025-08-02T08:40:30.161790Z","shell.execute_reply.started":"2025-08-02T08:40:27.020363Z","shell.execute_reply":"2025-08-02T08:40:30.160914Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (6.0.2)\nyolo_params.yaml has been updated successfully!\n{'train': 'data/train/images', 'val': 'data/val/images', 'test': 'data/test', 'nc': 3, 'names': ['FireExtinguisher', 'ToolBox', 'OxygenTank'], 'path': '/kaggle/input/falcon/HackByte_Dataset/data'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"#-------------------------------------------------------------------\n# STEP 1: INSTALL DEPENDENCIES\n#-------------------------------------------------------------------\nprint(\"Installing dependencies...\")\n!pip install ultralytics -q\n!pip install pyyaml -q\nprint(\"Installation complete!\")\n\n#-------------------------------------------------------------------\n# STEP 2: PREPARE A CLEAN WRITABLE DIRECTORY\n#-------------------------------------------------------------------\nimport os\nimport shutil\n\nworking_dir = '/kaggle/working/'\n\nprint(f\"Cleaning contents of {working_dir}...\")\nif os.path.exists(working_dir):\n    for item in os.listdir(working_dir):\n        item_path = os.path.join(working_dir, item)\n        try:\n            if os.path.isfile(item_path) or os.path.islink(item_path):\n                os.unlink(item_path)\n            elif os.path.isdir(item_path):\n                shutil.rmtree(item_path)\n        except Exception as e:\n            print(f'Failed to delete {item_path}. Reason: {e}')\nelse:\n    os.makedirs(working_dir)\nprint(\"Working directory is clean.\")\n\n#-------------------------------------------------------------------\n# STEP 3: COPY NECESSARY FILES\n#-------------------------------------------------------------------\nsource_dir = '/kaggle/input/falcon/HackByte_Dataset'\nfiles_to_copy = ['yolo_params.yaml', 'train.py', 'predict.py']\nfor file_name in files_to_copy:\n    shutil.copy(os.path.join(source_dir, file_name), os.path.join(working_dir, file_name))\nprint(f\"Copied essential files to {working_dir}\")\n\n#-------------------------------------------------------------------\n# STEP 4: MODIFY THE YAML CONFIGURATION FILE << THE IMPORTANT FIX\n#-------------------------------------------------------------------\nimport yaml\n\nyaml_file_path = os.path.join(working_dir, 'yolo_params.yaml')\n\n# Load the existing yaml file\nwith open(yaml_file_path, 'r') as file:\n    yolo_config = yaml.safe_load(file)\n\n# --- CORRECT THE PATHS ---\nyolo_config['path'] = '/kaggle/input/falcon/HackByte_Dataset/data'\nyolo_config['train'] = 'train/images'  # <<< CORRECTED LINE\nyolo_config['val'] = 'val/images'      # <<< CORRECTED LINE\nyolo_config['test'] = 'test/images'    # <<< CORRECTED LINE (assuming test images are also in an 'images' folder)\n\n# --- ADD OTHER TRAINING PARAMETERS ---\nyolo_config['imgsz'] = 640\nyolo_config['batch'] = 16\nyolo_config['epochs'] = 50 # Let's start with 50 epochs\n\n# --- ENSURE CLASS NAMES ARE CORRECT ---\nyolo_config['nc'] = 3\nyolo_config['names'] = ['FireExtinguisher', 'ToolBox', 'OxygenTank'] # From your log\n\n\n# Save the changes back to the file\nwith open(yaml_file_path, 'w') as file:\n    yaml.dump(yolo_config, file, sort_keys=False) # sort_keys=False keeps the order nice\n\nprint(\"\\n--- Updated yolo_params.yaml content ---\")\nwith open(yaml_file_path, 'r') as file:\n    print(file.read())\nprint(\"----------------------------------------\")\n\n#-------------------------------------------------------------------\n# STEP 5: NAVIGATE AND RUN TRAINING\n#-------------------------------------------------------------------\n%cd /kaggle/working/\n\nprint(f\"\\nCurrent directory: {os.getcwd()}\")\nprint(\"\\nStarting model training...\")\n\n# Run the training script. It will use the fully corrected yolo_params.yaml.\n!python train.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T20:38:32.239631Z","iopub.execute_input":"2025-08-01T20:38:32.239957Z","iopub.status.idle":"2025-08-01T20:41:22.341476Z","shell.execute_reply.started":"2025-08-01T20:38:32.239928Z","shell.execute_reply":"2025-08-01T20:41:22.340591Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\nInstallation complete!\nCleaning contents of /kaggle/working/...\nWorking directory is clean.\nCopied essential files to /kaggle/working/\n\n--- Updated yolo_params.yaml content ---\ntrain: train/images\nval: val/images\ntest: test/images\nnc: 3\nnames:\n- FireExtinguisher\n- ToolBox\n- OxygenTank\npath: /kaggle/input/falcon/HackByte_Dataset/data\nimgsz: 640\nbatch: 16\nepochs: 50\n\n----------------------------------------\n/kaggle/working\n\nCurrent directory: /kaggle/working\n\nStarting model training...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov\nUltralytics 8.3.172 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/yolo_params.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.0001, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/kaggle/working/yolov8s.pt, momentum=0.2, mosaic=0.1, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralyti\nOverriding model.yaml nc=80 with nc=3\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \nModel summary: 129 layers, 11,136,761 parameters, 11,136,745 gradients, 28.7 GFLOPs\n\nTransferred 349/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo1\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 98.7±48.1 MB/s, size: 2900.0 KB)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/input/falcon/HackByte_Dataset/data/train/labels... 846 i\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mtrain: \u001b[0mCache directory /kaggle/input/falcon/HackByte_Dataset/data/train is not writeable, cache not saved.\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 158.6±23.8 MB/s, size: 3015.9 KB)\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/input/falcon/HackByte_Dataset/data/val/labels... 154 image\u001b[0m\nWARNING ⚠️ \u001b[34m\u001b[1mval: \u001b[0mCache directory /kaggle/input/falcon/HackByte_Dataset/data/val is not writeable, cache not saved.\nPlotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001, momentum=0.2) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 5 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        1/5      3.69G       1.14      3.551      1.251         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.34      0.419      0.296      0.176\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        2/5      3.91G      1.312      3.294      1.335         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.37      0.454       0.37      0.203\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        3/5      3.95G      1.286        3.3      1.277         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.572      0.382      0.419      0.192\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        4/5      3.98G      1.052      1.699      1.115         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.932      0.605      0.773      0.609\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n        5/5      4.02G      0.751     0.9259     0.9686         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.941      0.833      0.892       0.75\n\n5 epochs completed in 0.034 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 22.5MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 22.5MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics 8.3.172 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 11,126,745 parameters, 0 gradients, 28.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  m\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n                   all        154        206      0.947       0.83      0.892      0.751\n      FireExtinguisher         67         67      0.934       0.94      0.943      0.774\n               ToolBox         60         60      0.942      0.833      0.899      0.827\n            OxygenTank         79         79      0.966      0.717      0.836      0.652\nSpeed: 0.2ms preprocess, 2.7ms inference, 0.0ms loss, 3.5ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"#-------------------------------------------------------------------\n# STEP 1: INSTALL DEPENDENCIES\n#-------------------------------------------------------------------\nprint(\"Installing dependencies...\")\n!pip install ultralytics -q\n!pip install pyyaml -q\n!pip install opencv-python -q # For image processing\nprint(\"Installation complete!\")\n\n#-------------------------------------------------------------------\n# STEP 2: SETUP DIRECTORIES AND COPY FILES\n#-------------------------------------------------------------------\nimport os\nimport shutil\nimport cv2\nimport numpy as np\nimport random\n\n# --- Basic Setup ---\nsource_data_dir = '/kaggle/input/falcon/HackByte_Dataset/data'\nworking_dir = '/kaggle/working/'\nnew_data_dir = os.path.join(working_dir, 'data_augmented')\n\n# --- Clean and Create Directories ---\nprint(\"Setting up working directories...\")\nif os.path.exists(new_data_dir):\n    shutil.rmtree(new_data_dir)\nos.makedirs(new_data_dir, exist_ok=True)\n\n# --- Copy the entire original dataset to our new location ---\nshutil.copytree(source_data_dir, new_data_dir, dirs_exist_ok=True)\nprint(\"Copied original dataset to new augmented directory.\")\n\n# --- Copy scripts ---\nsource_script_dir = '/kaggle/input/falcon/HackByte_Dataset'\nfiles_to_copy = ['yolo_params.yaml', 'train.py', 'predict.py']\nfor file_name in files_to_copy:\n    shutil.copy(os.path.join(source_script_dir, file_name), os.path.join(working_dir, file_name))\nprint(\"Copied scripts.\")\n\n\n#-------------------------------------------------------------------\n# STEP 3: GENERALIZED AUGMENTATION (ALL CLASSES)\n#-------------------------------------------------------------------\nprint(\"\\nStarting generalized data augmentation for ALL classes...\")\ntrain_images_path = os.path.join(new_data_dir, 'train', 'images')\ntrain_labels_path = os.path.join(new_data_dir, 'train', 'labels')\n\naugmentation_count = 0\nfor label_file in os.listdir(train_labels_path):\n    if not label_file.endswith('.txt'):\n        continue\n    \n    label_path = os.path.join(train_labels_path, label_file)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n\n    # Check if there are ANY objects in this image\n    if lines:\n        image_file = label_file.replace('.txt', '.png')\n        image_path = os.path.join(train_images_path, image_file)\n        if not os.path.exists(image_path):\n            image_file = label_file.replace('.txt', '.jpg')\n            image_path = os.path.join(train_images_path, image_file)\n\n        image = cv2.imread(image_path)\n        if image is None:\n            continue\n            \n        h, w, _ = image.shape\n        augmented_image = image.copy()\n        \n        for _ in range(random.randint(1, 3)):\n            box_w = int(w * random.uniform(0.1, 0.3))\n            box_h = int(h * random.uniform(0.1, 0.3))\n            x1 = random.randint(0, w - box_w)\n            y1 = random.randint(0, h - box_h)\n            cv2.rectangle(augmented_image, (x1, y1), (x1 + box_w, y1 + box_h), (0, 0, 0), -1)\n\n        new_image_filename = f\"aug_occlusion_{image_file}\"\n        new_label_filename = f\"aug_occlusion_{label_file}\"\n        cv2.imwrite(os.path.join(train_images_path, new_image_filename), augmented_image)\n        shutil.copy(label_path, os.path.join(train_labels_path, new_label_filename))\n        augmentation_count += 1\nprint(f\"Generated {augmentation_count} new images with artificial occlusion for all classes.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T08:40:38.089336Z","iopub.execute_input":"2025-08-02T08:40:38.089629Z","iopub.status.idle":"2025-08-02T08:43:22.073782Z","shell.execute_reply.started":"2025-08-02T08:40:38.089601Z","shell.execute_reply":"2025-08-02T08:43:22.072828Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\nInstallation complete!\nSetting up working directories...\nCopied original dataset to new augmented directory.\nCopied scripts.\n\nStarting generalized data augmentation for ALL classes...\nGenerated 841 new images with artificial occlusion for all classes.\n\n--- Running training on AUGMENTED dataset ---\nYAML Configuration:\ntrain: train/images\nval: val/images\ntest: test/images\nnc: 3\nnames:\n- FireExtinguisher\n- ToolBox\n- OxygenTank\npath: /kaggle/working/data_augmented\nimgsz: 640\nbatch: 16\nepochs: 125\npatience: 20\n\n---------------------------------------------\n/kaggle/working\nusage: train.py [-h] [--epochs EPOCHS] [--mosaic MOSAIC]\n                [--optimizer OPTIMIZER] [--momentum MOMENTUM] [--lr0 LR0]\n                [--lrf LRF] [--single_cls SINGLE_CLS]\ntrain.py: error: unrecognized arguments: --patience=20\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#-------------------------------------------------------------------\n# STEP 4: CONFIGURE AND TRAIN ON THE AUGMENTED DATASET\n#-------------------------------------------------------------------\nimport yaml\n\nyaml_file_path = os.path.join(working_dir, 'yolo_params.yaml')\n\nwith open(yaml_file_path, 'r') as file:\n    yolo_config = yaml.safe_load(file)\n\n# --- Point to our new augmented dataset ---\nyolo_config['path'] = new_data_dir # <<< CRITICAL CHANGE\nyolo_config['train'] = 'train/images'\nyolo_config['val'] = 'val/images'\nyolo_config['test'] = 'test/images'\n\n# --- Other training parameters ---\nyolo_config['imgsz'] = 640\nyolo_config['batch'] = 16\nyolo_config['epochs'] = 125 # Train a bit longer on the bigger dataset\n\n# --- Save the changes ---\nwith open(yaml_file_path, 'w') as file:\n    yaml.dump(yolo_config, file, sort_keys=False)\n\nprint(\"\\n--- Running training on AUGMENTED dataset ---\")\nprint(\"YAML Configuration:\")\nwith open(yaml_file_path, 'r') as file:\n    print(file.read())\nprint(\"---------------------------------------------\")\n\n# --- Navigate and Run ---\n%cd /kaggle/working/\n!python train.py --epochs 125 --mosaic 1.0 --lr0 0.0008 ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T08:43:49.641004Z","iopub.execute_input":"2025-08-02T08:43:49.641658Z","iopub.status.idle":"2025-08-02T10:25:19.272581Z","shell.execute_reply.started":"2025-08-02T08:43:49.641624Z","shell.execute_reply":"2025-08-02T10:25:19.271675Z"}},"outputs":[{"name":"stdout","text":"\n--- Running training on AUGMENTED dataset ---\nYAML Configuration:\ntrain: train/images\nval: val/images\ntest: test/images\nnc: 3\nnames:\n- FireExtinguisher\n- ToolBox\n- OxygenTank\npath: /kaggle/working/data_augmented\nimgsz: 640\nbatch: 16\nepochs: 125\npatience: 20\n\n---------------------------------------------\n/kaggle/working\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov\nUltralytics 8.3.172 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/yolo_params.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=125, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0008, lrf=0.0001, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/kaggle/working/yolov8s.pt, momentum=0.2, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralyti\nOverriding model.yaml nc=80 with nc=3\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \nModel summary: 129 layers, 11,136,761 parameters, 11,136,745 gradients, 28.7 GFLOPs\n\nTransferred 349/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\nDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo1\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2167.2±1520.7 MB/s, size: 2297.8 KB)\n\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/data_augmented/train/labels... 1687 images, 5 ba\u001b[0m\n\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/data_augmented/train/labels.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 987.7±1142.8 MB/s, size: 3015.9 KB)\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/data_augmented/val/labels... 154 images, 0 backgro\u001b[0m\n\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/data_augmented/val/labels.cache\nPlotting labels to runs/detect/train/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.0008, momentum=0.2) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/train\u001b[0m\nStarting training for 125 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      1/125       3.7G      1.076      1.814       1.22         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.78      0.696      0.761      0.561\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      2/125      3.93G      1.104      1.472      1.208         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.858      0.707      0.762      0.603\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      3/125      3.96G      1.154      1.289      1.213         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.79      0.752      0.817      0.654\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      4/125         4G      1.076       1.21      1.181         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.93      0.771      0.866      0.625\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      5/125      4.04G      1.029     0.9628      1.148         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.948      0.777      0.863      0.653\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      6/125      4.07G     0.9366     0.8842      1.098         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.902       0.83      0.892       0.72\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      7/125      4.11G     0.9098     0.8206      1.082         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.959      0.836      0.902      0.771\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      8/125      4.15G     0.8561      0.736      1.055         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.955      0.834      0.894       0.75\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n      9/125      4.18G     0.8581     0.7494      1.053         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.928      0.853      0.904      0.723\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     10/125      4.22G     0.7949     0.7088      1.031         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.937      0.864      0.897      0.758\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     11/125      4.25G     0.7749     0.6689      1.025         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.907      0.887      0.912      0.772\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     12/125      4.29G     0.7869     0.6865       1.03         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.926      0.854      0.917      0.779\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     13/125      4.33G     0.7571     0.6484      1.016         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.94      0.837      0.891      0.786\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     14/125      4.37G     0.7325     0.6218     0.9975         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.971      0.896      0.935       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     15/125       4.4G      0.724     0.6111     0.9895         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.976      0.856      0.927      0.819\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     16/125      4.44G     0.7024     0.5844     0.9937         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.981      0.883      0.925      0.803\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     17/125      4.47G     0.6849     0.5726     0.9832         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.972      0.886      0.929      0.818\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     18/125      4.51G     0.6724     0.5773     0.9703         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.959      0.829       0.91      0.816\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     19/125      4.55G     0.6911     0.5828     0.9871         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.965      0.866      0.925      0.826\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     20/125      4.61G     0.6709     0.5584     0.9762         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.994      0.885      0.935      0.832\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     21/125      4.68G     0.6709     0.5575     0.9684         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.97      0.867      0.918       0.81\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     22/125      4.76G     0.6452     0.5253     0.9654         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.973       0.89      0.933      0.835\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     23/125      4.82G     0.6394     0.5194      0.956         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.959        0.9      0.927      0.827\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     24/125      4.86G     0.6338     0.5155     0.9584         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.939      0.877      0.916      0.807\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     25/125      4.93G     0.6326     0.5088      0.956         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.97       0.89      0.929      0.824\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     26/125         5G     0.5981     0.4873     0.9372         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.962      0.899      0.929      0.847\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     27/125      5.07G     0.6133     0.4911     0.9519         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.98      0.893      0.934      0.845\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     28/125      5.14G     0.6159     0.4953     0.9504         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.954      0.894      0.925      0.835\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     29/125      5.21G     0.5984     0.4773      0.937         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.958      0.897      0.937       0.84\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     30/125      5.28G     0.5855     0.4768     0.9372         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.976      0.899      0.933       0.85\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     31/125      5.31G     0.5996     0.4823     0.9474         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.96      0.907      0.938       0.86\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     32/125      5.38G     0.5706     0.4653     0.9244         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.982        0.9      0.937      0.853\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     33/125      5.45G     0.5651     0.4572     0.9274         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.977      0.903      0.936      0.856\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     34/125      5.49G      0.565     0.4594     0.9298         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.95      0.904      0.933      0.846\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     35/125      5.56G     0.5721     0.4628     0.9274         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.984      0.888      0.931      0.849\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     36/125      5.63G     0.5609     0.4585     0.9201         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.951      0.905      0.934      0.862\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     37/125       5.7G     0.5452     0.4414     0.9152         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.971      0.905      0.928      0.854\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     38/125      5.77G     0.5412     0.4338     0.9165         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.967      0.917      0.936      0.856\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     39/125      5.84G     0.5379     0.4345     0.9095         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.972      0.908      0.932      0.851\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     40/125      5.87G     0.5346     0.4243     0.9086         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.987        0.9      0.933      0.846\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     41/125      5.94G     0.5297     0.4282     0.9077         12        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.986      0.891      0.938      0.865\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     42/125      6.01G     0.5119     0.4014     0.9006         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.97      0.904      0.944      0.875\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     43/125      6.08G     0.4975     0.3976     0.8981         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.991      0.918      0.951      0.876\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     44/125      6.15G     0.5047     0.4011     0.9011         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.975      0.905      0.938      0.869\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     45/125      6.22G     0.5011     0.3952     0.9014         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.977      0.909      0.947       0.87\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     46/125      6.29G     0.4924     0.3938     0.8939         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.963      0.886      0.932      0.856\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     47/125      6.33G     0.5036     0.3925     0.8947         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.976      0.906      0.944      0.872\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     48/125      6.39G     0.4948     0.3865     0.8984         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.993      0.894      0.941      0.869\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     49/125      6.46G     0.4783     0.3842     0.8912         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.972      0.906       0.95      0.868\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     50/125      6.53G     0.4901     0.3957     0.8942         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.983      0.893      0.938      0.863\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     51/125      6.61G     0.4846     0.3823     0.8917         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.983        0.9      0.943       0.87\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     52/125      6.67G     0.4623     0.3643      0.879         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.982        0.9      0.941      0.869\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     53/125      6.71G     0.4803     0.3841     0.8963         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.993      0.885       0.94      0.869\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     54/125      6.78G     0.4665     0.3756     0.8925         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.985      0.898      0.955      0.876\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     55/125      6.85G      0.461     0.3623     0.8797         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.993      0.897      0.942      0.876\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     56/125      6.92G     0.4655     0.3678     0.8884         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.996      0.897       0.95      0.877\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     57/125      6.99G     0.4517     0.3536     0.8778         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.969      0.916      0.945      0.878\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     58/125      7.06G     0.4619      0.351     0.8843         27        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.985      0.896      0.942      0.881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     59/125      7.13G     0.4466     0.3508     0.8789         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.984      0.904      0.941      0.878\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     60/125      7.16G     0.4521     0.3537     0.8838         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.991      0.889       0.94      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     61/125      7.23G     0.4478      0.356      0.881         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.983      0.911      0.947      0.888\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     62/125       7.3G     0.4414     0.3424     0.8765         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.987        0.9      0.946      0.889\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     63/125      7.37G     0.4436     0.3454     0.8824         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.988      0.902      0.951      0.888\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     64/125      4.01G     0.4374      0.338     0.8807         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.988      0.903      0.945      0.881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     65/125      4.01G     0.4356     0.3298     0.8778         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.987      0.899      0.948      0.892\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     66/125      4.01G     0.4352     0.3307     0.8741         24        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.981      0.908      0.946      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     67/125      4.01G     0.4383     0.3365     0.8797         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.973      0.912       0.95      0.894\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     68/125      4.01G     0.4208      0.314     0.8726         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.992      0.897      0.943      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     69/125      4.01G     0.4328     0.3352     0.8783         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.986      0.891      0.945      0.894\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     70/125      4.04G     0.4129     0.3194     0.8672         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.985      0.905      0.944      0.885\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     71/125      4.07G     0.4141     0.3125     0.8653         26        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.994        0.9      0.949      0.897\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     72/125      4.11G     0.4124     0.3151     0.8633         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.972      0.905      0.949       0.89\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     73/125      4.15G     0.4069      0.317     0.8695         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.969      0.918      0.948      0.889\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     74/125      4.18G     0.4094     0.3065     0.8672         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.953      0.916       0.95      0.891\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     75/125      4.22G     0.4137     0.3135      0.868         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.968      0.919       0.95      0.892\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     76/125      4.26G     0.4107     0.3114      0.872         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.988        0.9      0.951      0.893\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     77/125      4.29G     0.4035     0.3073     0.8682         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.992       0.91      0.955      0.904\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     78/125      4.33G     0.3956     0.2977     0.8666         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.988      0.901       0.95      0.894\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     79/125      4.37G     0.3933     0.2995     0.8639         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.984      0.904      0.947      0.894\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     80/125      4.43G     0.3905     0.2949     0.8627         29        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.986      0.894      0.951      0.897\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     81/125       4.5G     0.4008     0.2966      0.866         26        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.992      0.891       0.95      0.903\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     82/125      4.58G     0.3881     0.3033     0.8663         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.99      0.904      0.949      0.898\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     83/125      4.64G     0.3926     0.2955     0.8676         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.981      0.921       0.95      0.893\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     84/125      4.71G     0.3743     0.2863      0.853         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.985      0.908      0.948      0.899\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     85/125      4.78G     0.3743     0.2846     0.8582         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.991      0.904      0.948      0.895\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     86/125      4.85G     0.3775      0.282     0.8522         24        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.989      0.904      0.949        0.9\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     87/125      4.89G     0.3684     0.2719     0.8554         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.993      0.908      0.952      0.906\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     88/125      4.96G     0.3688     0.2772     0.8524         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.989      0.904      0.948      0.903\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     89/125      5.03G     0.3608     0.2752      0.848         23        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.986      0.909      0.952      0.905\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     90/125       5.1G     0.3671     0.2732      0.856         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.971      0.926       0.95      0.901\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     91/125      5.17G     0.3741     0.2815     0.8585         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.989      0.903      0.946      0.902\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     92/125      5.24G     0.3696      0.279      0.861         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.989      0.899      0.942      0.899\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     93/125      5.27G     0.3654     0.2712     0.8591         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.991      0.899      0.944        0.9\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     94/125      5.34G     0.3586     0.2673     0.8515         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.989      0.906      0.947      0.903\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     95/125      5.41G     0.3588     0.2644     0.8578         24        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.979      0.914       0.95      0.903\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     96/125      5.48G     0.3522     0.2602     0.8487         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.999      0.915      0.953      0.907\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     97/125      5.55G     0.3451     0.2611     0.8525         20        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.979      0.921      0.952      0.903\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     98/125      5.62G     0.3478     0.2594     0.8509         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.991      0.902      0.953      0.904\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n     99/125      5.65G     0.3469      0.255     0.8504         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.988      0.916      0.955      0.908\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    100/125      5.73G     0.3344     0.2501     0.8468         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.996      0.912      0.952      0.903\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    101/125      5.79G     0.3316      0.246     0.8442         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.996      0.919      0.957      0.909\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    102/125      5.86G     0.3325     0.2479     0.8456         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.983      0.922      0.952      0.907\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    103/125      5.93G     0.3376       0.25     0.8504         16        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.993      0.907      0.952      0.909\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    104/125         6G     0.3369     0.2511     0.8496         25        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.974      0.912       0.95      0.904\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    105/125      6.07G     0.3327     0.2445     0.8451         17        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.984       0.91       0.95      0.905\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    106/125      6.11G     0.3334     0.2495     0.8503         15        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.993      0.907      0.949      0.908\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    107/125      6.18G     0.3237     0.2458     0.8462         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.988      0.904      0.949      0.907\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    108/125      6.25G      0.328     0.2444     0.8457         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.979      0.912       0.95      0.905\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    109/125      6.32G     0.3235     0.2442     0.8468         26        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.979      0.914      0.954      0.908\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    110/125      6.38G     0.3146     0.2349     0.8405         18        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.988      0.909      0.955      0.904\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    111/125      6.46G      0.314     0.2359     0.8405         13        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.984      0.907      0.952       0.91\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    112/125      6.53G     0.3169      0.236     0.8403         22        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.991       0.91      0.952      0.908\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    113/125      6.56G     0.3243     0.2398     0.8498         19        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.991        0.9      0.953      0.908\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    114/125      6.63G     0.3139     0.2355     0.8402         21        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.971       0.92      0.951      0.906\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    115/125       6.7G     0.3111     0.2317     0.8429         14        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.982      0.918      0.953      0.903\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    116/125      6.84G     0.2381      0.175     0.7859          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.978      0.913      0.954      0.903\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    117/125      6.87G     0.2318     0.1704     0.7755          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206       0.99       0.91      0.952      0.904\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    118/125      6.91G      0.223     0.1654     0.7777         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.979      0.917      0.953      0.908\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    119/125      6.95G     0.2282     0.1667     0.7735          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.981      0.906      0.952      0.907\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    120/125      7.01G     0.2218     0.1632     0.7737          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.976      0.918      0.952      0.907\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    121/125      7.08G     0.2199     0.1619     0.7753          8        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.977      0.914      0.952      0.909\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    122/125      7.15G     0.2163     0.1611     0.7712          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.977      0.916      0.954      0.911\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    123/125      7.22G     0.2122     0.1609     0.7733          9        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.971      0.911      0.953      0.909\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    124/125      7.29G     0.2132      0.158     0.7702          6        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.964      0.915      0.951      0.906\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n    125/125      7.33G      0.216     0.1612      0.783         11        640: 1\n                 Class     Images  Instances      Box(P          R      mAP50  m\n                   all        154        206      0.972      0.911       0.95      0.907\n\n125 epochs completed in 1.675 hours.\nOptimizer stripped from runs/detect/train/weights/last.pt, 22.5MB\nOptimizer stripped from runs/detect/train/weights/best.pt, 22.5MB\n\nValidating runs/detect/train/weights/best.pt...\nUltralytics 8.3.172 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 11,126,745 parameters, 0 gradients, 28.4 GFLOPs\n                 Class     Images  Instances      Box(P          R      mAP50  m\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n                   all        154        206      0.977      0.916      0.954      0.912\n      FireExtinguisher         67         67      0.985      0.953      0.984      0.946\n               ToolBox         60         60       0.96        0.9      0.928      0.911\n            OxygenTank         79         79      0.986      0.896       0.95      0.879\nSpeed: 0.2ms preprocess, 3.2ms inference, 0.0ms loss, 5.5ms postprocess per image\nResults saved to \u001b[1mruns/detect/train\u001b[0m\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ===================================================================\n# STEP 1: INSTALL DEPENDENCIES\n# ===================================================================\nprint(\"Installing dependencies...\")\n!pip install ultralytics -q\n!pip install pyyaml -q\n!pip install opencv-python -q\nprint(\"Installation complete!\")\n\n# ===================================================================\n# STEP 2: SETUP DIRECTORIES\n# ===================================================================\nimport os\nimport shutil\nimport cv2\nimport numpy as np\nimport random\nimport yaml\nimport torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\nfrom ultralytics.nn.modules import Conv, C2f, SPPF\nfrom ultralytics.nn.tasks import Detect\nimport math\nfrom ultralytics.utils.loss import BboxLoss\n\nprint(\"\\nSetting up working directories...\")\nworking_dir = '/kaggle/working/'\nsource_data_dir = '/kaggle/input/falcon/HackByte_Dataset/data'\nnew_data_dir = os.path.join(working_dir, 'data_augmented')\n\nif os.path.exists(new_data_dir):\n    shutil.rmtree(new_data_dir)\nshutil.copytree(source_data_dir, new_data_dir)\nprint(\"Copied original dataset to new augmented directory.\")\n\nshutil.copytree('/kaggle/input/falcon/HackByte_Dataset', working_dir, dirs_exist_ok=True)\nprint(\"Copied essential files to working directory.\")\n\n# ===================================================================\n# STEP 3: DYNAMICALLY CREATE CUSTOM MODULES FILE AND IMPORT\n# ===================================================================\nprint(\"\\nDynamically creating custom_modules.py...\")\ncustom_modules_code = \"\"\"\nimport torch\nimport torch.nn as nn\nimport math\nfrom ultralytics.nn.modules import Conv, C2f, SPPF\nfrom ultralytics.nn.tasks import Detect\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n                                nn.ReLU(),\n                                nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'\n        padding = 3 if kernel_size == 7 else 1\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        x = self.conv1(x)\n        return self.sigmoid(x)\n\nclass CBAM(nn.Module):\n    def __init__(self, c1, c2):\n        super().__init__()\n        self.channel_attention = ChannelAttention(c1)\n        self.spatial_attention = SpatialAttention()\n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        x = x * self.spatial_attention(x)\n        return x\n\nclass PConv(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        self.conv = nn.Conv2d(dim, dim, 3, padding=1, groups=dim)\n        self.act = nn.ReLU(inplace=True)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out_channel = x.shape[1] // 2\n        x[:, :out_channel, :, :] = self.conv(x[:, :out_channel, :, :])\n        return self.act(x)\n\ndef bbox_iou_siou(box1, box2, eps=1e-7):\n    b1_x1, b1_y1, b1_x2, b1_y2 = box1.chunk(4, -1)\n    b2_x1, b2_y1, b2_x2, b2_y2 = box2.chunk(4, -1)\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n    union = w1 * h1 + w2 * h2 - inter + eps\n    iou = inter / union\n    c_x, c_y = (b1_x1 + b1_x2) / 2, (b1_y1 + b1_y2) / 2\n    c_x_gt, c_y_gt = (b2_x1 + b2_x2) / 2, (b2_y1 + b2_y2) / 2\n    cw, ch = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1), torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n    sigma = torch.pow(c_x - c_x_gt, 2) + torch.pow(c_y - c_y_gt, 2)\n    sin_alpha_1 = torch.abs(c_y - c_y_gt) / torch.sqrt(sigma + eps)\n    sin_alpha_2 = torch.abs(c_x - c_x_gt) / torch.sqrt(sigma + eps)\n    threshold = math.pi / 4\n    angle_cost = torch.sin(2 * torch.arcsin(torch.sin(torch.abs(torch.asin(sin_alpha_1) - threshold))))\n    rho_x = (c_x_gt - c_x) / cw\n    rho_y = (c_y_gt - c_y) / ch\n    gamma = 2 - angle_cost\n    distance_cost = 1 - torch.exp(-gamma * (torch.pow(rho_x, 2) + torch.pow(rho_y, 2)))\n    omega_w = torch.abs(w1 - w2) / torch.max(w1, w2)\n    omega_h = torch.abs(h1 - h2) / torch.max(h1, h2)\n    shape_cost = torch.pow(1 - torch.exp(-1 * omega_w), 4) + torch.pow(1 - torch.exp(-1 * omega_h), 4)\n    siou_loss = 1 - iou + distance_cost + shape_cost\n    return siou_loss\n\"\"\"\nwith open(os.path.join(working_dir, 'custom_modules.py'), 'w') as f:\n    f.write(custom_modules_code)\n\nprint(\"Custom modules created.\")\n\n\n# ===================================================================\n# STEP 4: CREATE CUSTOM MODEL YAML FILE\n# ===================================================================\nyolov8s_custom_yaml = \"\"\"\n# YOLOv8s-custom model with CBAM, PConv, and Dropout\n# This YAML now references modules directly by their name.\nnc: 3\ndepth_multiple: 0.33\nwidth_multiple: 0.50\n\nbackbone:\n  [[-1, 1, Conv, [32, 3, 2]],\n   [-1, 1, Conv, [64, 3, 2]],\n   [-1, 1, C2f, [64, 64, 1, True]],\n   [-1, 1, Conv, [128, 3, 2]],\n   [-1, 2, C2f, [128, 128, 2, True]],\n   \n   # Custom blocks\n   [-1, 1, CBAM, [128, 128]],\n   [-1, 1, PConv, [128]],\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [-1, 2, C2f, [256, 256, 2, True]],\n   [-1, 1, Conv, [512, 3, 2]],\n   [-1, 1, C2f, [512, 512, 1, True]],\n   [-1, 1, SPPF, [512, 512, 5]]\n  ]\n\nhead:\n  [[-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, torch.cat, [1]],\n   [-1, 1, C2f, [768, 256, 1]],\n   \n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, torch.cat, [1]],\n   [-1, 1, C2f, [384, 128, 1]],\n   \n   [-1, 1, nn.Dropout2d, [0.1]],\n   \n   [-1, 1, Conv, [128, 128, 3, 2]],\n   [[-1, 12], 1, torch.cat, [1]],\n   [-1, 1, C2f, [384, 256, 1]],\n   \n   [-1, 1, Conv, [256, 256, 3, 2]],\n   [[-1, 9], 1, torch.cat, [1]],\n   [-1, 1, C2f, [768, 512, 1]],\n   \n   [[15, 18, 21], 1, Detect, [3, [128, 256, 512]]]\n  ]\n\"\"\"\nwith open(os.path.join(working_dir, 'yolov8s-custom.yaml'), 'w') as f:\n    f.write(yolov8s_custom_yaml)\nprint(\"Custom model YAML file created.\")\n\n# ===================================================================\n# STEP 5: GENERALIZED DATA AUGMENTATION\n# ===================================================================\nprint(\"\\nStarting generalized data augmentation for ALL classes...\")\ntrain_images_path = os.path.join(new_data_dir, 'train', 'images')\ntrain_labels_path = os.path.join(new_data_dir, 'train', 'labels')\n\naugmentation_count = 0\nfor label_file in os.listdir(train_labels_path):\n    if not label_file.endswith('.txt'):\n        continue\n    \n    label_path = os.path.join(train_labels_path, label_file)\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n\n    if lines:\n        image_file = label_file.replace('.txt', '.png')\n        image_path = os.path.join(train_images_path, image_file)\n        if not os.path.exists(image_path):\n            image_file = label_file.replace('.txt', '.jpg')\n            image_path = os.path.join(train_images_path, image_file)\n\n        image = cv2.imread(image_path)\n        if image is None:\n            continue\n            \n        h, w, _ = image.shape\n        augmented_image = image.copy()\n        \n        for _ in range(random.randint(1, 3)):\n            box_w = int(w * random.uniform(0.1, 0.3))\n            box_h = int(h * random.uniform(0.1, 0.3))\n            x1 = random.randint(0, w - box_w)\n            y1 = random.randint(0, h - box_h)\n            cv2.rectangle(augmented_image, (x1, y1), (x1 + box_w, y1 + box_h), (0, 0, 0), -1)\n\n        new_image_filename = f\"aug_occlusion_{image_file}\"\n        new_label_filename = f\"aug_occlusion_{label_file}\"\n        cv2.imwrite(os.path.join(train_images_path, new_image_filename), augmented_image)\n        shutil.copy(label_path, os.path.join(train_labels_path, new_label_filename))\n        augmentation_count += 1\nprint(f\"Generated {augmentation_count} new images with artificial occlusion for all classes.\")\n\n# ===================================================================\n# STEP 6: CONFIGURE AND TRAIN THE NEW MODEL (NATIVE PYTHON)\n# This is the key to a successful run.\n# ===================================================================\n# We import our custom modules here to ensure they are in the current namespace.\nimport custom_modules\nfrom custom_modules import CBAM, PConv, bbox_iou_siou\nprint(\"\\nCustom modules imported successfully into the current session.\")\n\nprint(\"\\nDynamically replacing BboxLoss with SIoU loss.\")\nBboxLoss.iou = bbox_iou_siou\n\nprint(\"\\nStarting model training with custom architecture and SIoU loss...\")\n\n# Change the current directory so YOLO can find our custom_modules.py file\nos.chdir(working_dir)\n\n# Initialize YOLO model with our custom YAML file\nmodel = YOLO(os.path.join(working_dir, 'yolov8s-custom.yaml'))\n\n# Train the model using the native Python API, which works in the current session.\nmodel.train(\n    data=os.path.join(new_data_dir, 'yolo_params.yaml'),\n    epochs=100,\n    imgsz=640,\n    project='runs/detect',\n    name='custom_model_v3'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T22:09:12.174344Z","iopub.execute_input":"2025-08-01T22:09:12.175187Z","iopub.status.idle":"2025-08-01T22:12:02.347029Z","shell.execute_reply.started":"2025-08-01T22:09:12.175156Z","shell.execute_reply":"2025-08-01T22:12:02.345909Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\nInstallation complete!\n\nSetting up working directories...\nCopied original dataset to new augmented directory.\nCopied essential files to working directory.\n\nDynamically creating custom_modules.py...\nCustom modules created.\nCustom model YAML file created.\n\nStarting generalized data augmentation for ALL classes...\nGenerated 841 new images with artificial occlusion for all classes.\n\nCustom modules imported successfully into the current session.\n\nDynamically replacing BboxLoss with SIoU loss.\n\nStarting model training with custom architecture and SIoU loss...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1214191575.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;31m# Initialize YOLO model with our custom YAML file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yolov8s-custom.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;31m# Train the model using the native Python API, which works in the current session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Continue with default YOLO initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"RTDETR\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if RTDETR head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRTDETR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"os\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUBLAS_WORKSPACE_CONFIG\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\":4096:8\"\u001b[0m  \u001b[0;31m# to avoid deterministic warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".yml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, cfg, task, model, verbose)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mguess_model_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, ch, nc, verbose)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnc\u001b[0m  \u001b[0;31m# override YAML value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# model, savelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# default names dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mparse_model\u001b[0;34m(d, ch, verbose)\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"torchvision.ops.\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m         )  # get module\n\u001b[1;32m   1676\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'CBAM'"],"ename":"KeyError","evalue":"'CBAM'","output_type":"error"}],"execution_count":15},{"cell_type":"code","source":"# Import custom modules\nimport custom_modules\nfrom custom_modules import CBAM, PConv, bbox_iou_siou\n\n# Register for YOLO parsing inside ultralytics internals\nimport ultralytics.nn.tasks as tasks\ntasks.__dict__[\"CBAM\"] = CBAM\ntasks.__dict__[\"PConv\"] = PConv\n\nprint(\"\\nCustom modules imported successfully into the current session.\")\n\n# Replace default bbox loss with custom SIoU\nprint(\"\\nDynamically replacing BboxLoss with SIoU loss.\")\nBboxLoss.iou = bbox_iou_siou\n\nprint(\"\\nStarting model training with custom architecture and SIoU loss...\")\n\n# Change working directory\nos.chdir(working_dir)\n\n# Create YOLO model from custom YAML\nmodel = YOLO(os.path.join(working_dir, 'yolov8s-custom.yaml'))\n\n# Train\nmodel.train(\n    data=os.path.join(new_data_dir, 'yolo_params.yaml'),\n    epochs=100,\n    imgsz=640,\n    project='runs/detect',\n    name='custom_model_v3'\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T22:15:42.362338Z","iopub.execute_input":"2025-08-01T22:15:42.362668Z","iopub.status.idle":"2025-08-01T22:15:42.388144Z","shell.execute_reply.started":"2025-08-01T22:15:42.362643Z","shell.execute_reply":"2025-08-01T22:15:42.387196Z"}},"outputs":[{"name":"stdout","text":"\nCustom modules imported successfully into the current session.\n\nDynamically replacing BboxLoss with SIoU loss.\n\nStarting model training with custom architecture and SIoU loss...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/963540922.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Now YOLO will correctly recognize the CBAM and PConv modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYOLO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yolov8s-custom.yaml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# Train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/models/yolo/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;31m# Continue with default YOLO initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"RTDETR\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# if RTDETR head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mfrom\u001b[0m \u001b[0multralytics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRTDETR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"os\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUBLAS_WORKSPACE_CONFIG\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\":4096:8\"\u001b[0m  \u001b[0;31m# to avoid deterministic warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\".yml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36m_new\u001b[0;34m(self, cfg, task, model, verbose)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mguess_model_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_smart_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# build model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg, ch, nc, verbose)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nc\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnc\u001b[0m  \u001b[0;31m# override YAML value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# model, savelist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mf\"{i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"nc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m  \u001b[0;31m# default names dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/nn/tasks.py\u001b[0m in \u001b[0;36mparse_model\u001b[0;34m(d, ch, verbose)\u001b[0m\n\u001b[1;32m   1672\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torchvision\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"torchvision.ops.\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m             \u001b[0;32melse\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m         )  # get module\n\u001b[1;32m   1676\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'CBAM'"],"ename":"KeyError","evalue":"'CBAM'","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# ===================================================================\n# STEP 1: INSTALL DEPENDENCIES\n# ===================================================================\nprint(\"Installing dependencies...\")\n!pip install ultralytics -q\n!pip install pyyaml -q\n!pip install opencv-python -q\n!pip install timm -q \nprint(\"Installation complete!\")\n\n# ===================================================================\n# STEP 2: SETUP DIRECTORIES AND CUSTOM MODULES\n# ===================================================================\nimport os\nimport shutil\nimport cv2\nimport numpy as np\nimport random\nimport yaml\nimport torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\nfrom ultralytics.nn.modules import Conv, C2f, SPPF\nfrom ultralytics.nn.tasks import Detect\nimport math\nfrom ultralytics.utils.loss import BboxLoss\nimport timm\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T00:02:04.071248Z","iopub.status.idle":"2025-08-02T00:02:04.071615Z","shell.execute_reply.started":"2025-08-02T00:02:04.071444Z","shell.execute_reply":"2025-08-02T00:02:04.071461Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# STEP 1: INSTALL DEPENDENCIES\n# ===================================================================\nprint(\"Installing dependencies...\")\n!pip install ultralytics -q\n!pip install pyyaml -q\n!pip install opencv-python -q\nprint(\"Installation complete!\")\n\n# ===================================================================\n# STEP 2: SETUP, COPY, AND AUGMENT THE DATA (Our Winning Strategy)\n# ===================================================================\nimport os\nimport shutil\nimport cv2\nimport numpy as np\nimport random\nimport yaml\nimport torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\nfrom ultralytics.nn.modules import Conv, C2f, SPPF, Concat\nfrom ultralytics.nn.tasks import Detect\nfrom ultralytics.utils.loss import BboxLoss\nfrom ultralytics.utils.ops import xywh2xyxy\nfrom ultralytics.utils.metrics import bbox_iou\nimport math\n\n# --- Basic Setup ---\nsource_data_dir = '/kaggle/input/falcon/HackByte_Dataset/data'\nworking_dir = '/kaggle/working/'\nnew_data_dir = os.path.join(working_dir, 'data_augmented')\n\n# --- Clean and Create Directories ---\nprint(\"\\nSetting up working directories...\")\nif os.path.exists(new_data_dir):\n    shutil.rmtree(new_data_dir)\nshutil.copytree(source_data_dir, new_data_dir, dirs_exist_ok=True)\n\n# --- Perform Artificial Occlusion ---\nprint(\"\\nStarting advanced augmentation...\")\ntrain_images_path = os.path.join(new_data_dir, 'train', 'images')\ntrain_labels_path = os.path.join(new_data_dir, 'train', 'labels')\naugmentation_count = 0\nif not any(f.startswith('aug_occlusion_') for f in os.listdir(train_images_path)):\n    for label_file in os.listdir(train_labels_path):\n        if not label_file.endswith('.txt'): continue\n        with open(os.path.join(train_labels_path, label_file), 'r') as f:\n            lines = f.readlines()\n        if lines:\n            image_file = label_file.replace('.txt', '.png')\n            image_path = os.path.join(train_images_path, image_file)\n            if not os.path.exists(image_path):\n                image_file = label_file.replace('.txt', '.jpg')\n                image_path = os.path.join(train_images_path, image_file)\n            image = cv2.imread(image_path)\n            if image is None: continue\n            h, w, _ = image.shape\n            augmented_image = image.copy()\n            for _ in range(random.randint(1, 3)):\n                box_w, box_h = int(w*random.uniform(0.1,0.3)), int(h*random.uniform(0.1,0.3))\n                x1, y1 = random.randint(0, w-box_w), random.randint(0, h-box_h)\n                cv2.rectangle(augmented_image, (x1, y1), (x1+box_w, y1+box_h), (0,0,0), -1)\n            new_image_filename = f\"aug_occlusion_{image_file}\"\n            cv2.imwrite(os.path.join(train_images_path, new_image_filename), augmented_image)\n            shutil.copy(os.path.join(train_labels_path, label_file), os.path.join(train_labels_path, new_image_filename.replace('.png', '.txt').replace('.jpg', '.txt')))\n            augmentation_count += 1\n    print(f\"Generated {augmentation_count} new images with artificial occlusion.\")\nelse:\n    print(\"Augmented images already exist. Skipping generation.\")\n\n\n# ===================================================================\n# STEP 3: DEFINE AND MANUALLY INJECT CUSTOM MODULES\n# ===================================================================\nprint(\"\\nDefining custom modules and loss function...\")\n\n# --- CBAM Module Definition ---\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False), nn.ReLU(), nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        return self.sigmoid(avg_out + max_out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        padding = 3 if kernel_size == 7 else 1\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        return self.sigmoid(self.conv1(x))\n\nclass CBAM(nn.Module):\n    def __init__(self, c1, c2):\n        super().__init__()\n        self.channel_attention = ChannelAttention(c1)\n        self.spatial_attention = SpatialAttention()\n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        return x * self.spatial_attention(x)\n\n# --- FasterNet-like PConv Block ---\nclass PConv(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        self.conv = nn.Conv2d(dim // 2, dim // 2, 3, padding=1, groups=dim // 2)\n        self.act = nn.ReLU(inplace=True)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out_channel = x.shape[1] // 2\n        x[:, :out_channel, :, :] = self.conv(x[:, :out_channel, :, :])\n        return self.act(x)\n\n# --- SIoU Loss Function ---\ndef siou_loss(pred, target, eps=1e-7):\n    # This is a simplified SIoU function to replace the default in BboxLoss\n    b1_x1, b1_y1, b1_x2, b1_y2 = pred.chunk(4, -1)\n    b2_x1, b2_y1, b2_x2, b2_y2 = target.chunk(4, -1)\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n    union = w1 * h1 + w2 * h2 - inter + eps\n    iou = inter / union\n    c_x, c_y = (b1_x1 + b1_x2) / 2, (b1_y1 + b1_y2) / 2\n    c_x_gt, c_y_gt = (b2_x1 + b2_x2) / 2, (b2_y1 + b2_y2) / 2\n    cw, ch = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1), torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n    sigma = torch.pow(c_x - c_x_gt, 2) + torch.pow(c_y - c_y_gt, 2)\n    sin_alpha_1 = torch.abs(c_y - c_y_gt) / torch.sqrt(sigma + eps)\n    sin_alpha_2 = torch.abs(c_x - c_x_gt) / torch.sqrt(sigma + eps)\n    threshold = math.pi / 4\n    angle_cost = torch.sin(2 * torch.arcsin(torch.sin(torch.abs(torch.asin(sin_alpha_1) - threshold))))\n    rho_x = (c_x_gt - c_x) / cw\n    rho_y = (c_y_gt - c_y) / ch\n    gamma = 2 - angle_cost\n    distance_cost = 1 - torch.exp(-gamma * (torch.pow(rho_x, 2) + torch.pow(rho_y, 2)))\n    omega_w = torch.abs(w1 - w2) / torch.max(w1, w2)\n    omega_h = torch.abs(h1 - h2) / torch.max(h1, h2)\n    shape_cost = torch.pow(1 - torch.exp(-1 * omega_w), 4) + torch.pow(1 - torch.exp(-1 * omega_h), 4)\n    siou_loss = 1 - iou + distance_cost + shape_cost\n    return siou_loss\n\n# --- Manual Model Injection ---\nprint(\"Manually injecting custom layers into the YOLOv8s backbone...\")\nmodel = YOLO('yolov8s.pt')\nbackbone = model.model.model[:-1]\nhead = model.model.model[-1]\n\n# Inject CBAM and PConv after the C2f block at index 6\nbackbone.insert(7, CBAM(256, 256))\nbackbone.insert(8, PConv(256))\n\n# Rebuild the full model\nmodel.model.model = torch.nn.Sequential(*backbone, head)\nmodel.model.model.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\nmodel.to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\nprint(\"Model architecture modified successfully.\")\n\n\n# ===================================================================\n# STEP 4: CONFIGURE AND RUN TRAINING PROGRAMMATICALLY\n# ===================================================================\n# --- Create the data configuration file ---\ndata_yaml_path = os.path.join(working_dir, 'data_config.yaml')\nwith open(data_yaml_path, 'w') as f:\n    yaml.dump({\n        'path': new_data_dir,\n        'train': 'train/images',\n        'val': 'val/images',\n        'test': 'test/images',\n        'nc': 3,\n        'names': ['FireExtinguisher', 'ToolBox', 'OxygenTank']\n    }, f)\nprint(\"\\nCreated data configuration file.\")\n\n# --- Patch the loss function in the same session ---\nBboxLoss.iou = siou_loss\nprint(\"BboxLoss patched with SIoU loss function.\")\n\n# --- Run the training directly from Python ---\nprint(\"\\n--- Running FINAL POLISH training programmatically ---\")\nos.chdir(working_dir)\nmodel.train(\n    data=data_yaml_path,\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    mosaic=1.0,\n    lr0=0.0008,\n    cos_lr=True,\n    project='runs/detect',\n    name='Final_Model_Custom',\n    patience=25\n)\n\n# After training, run validation on the test set\nprint(\"\\n--- Running final validation on the test set ---\")\nmodel.val(split='test')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T00:08:04.702788Z","iopub.execute_input":"2025-08-02T00:08:04.703089Z","iopub.status.idle":"2025-08-02T00:11:42.176987Z","shell.execute_reply.started":"2025-08-02T00:08:04.703066Z","shell.execute_reply":"2025-08-02T00:11:42.175416Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\nInstallation complete!\n\nSetting up working directories...\n\nStarting advanced augmentation...\nGenerated 841 new images with artificial occlusion.\n\nDefining custom modules and loss function...\nManually injecting custom layers into the YOLOv8s backbone...\nModel architecture modified successfully.\n\nCreated data configuration file.\nBboxLoss patched with SIoU loss function.\n\n--- Running FINAL POLISH training programmatically ---\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/kaggle/working/data_config.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0008, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=Final_Model_Custom, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=25, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/Final_Model_Custom, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\nOverriding model.yaml nc=80 with nc=3\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \nModel summary: 129 layers, 11,136,761 parameters, 11,136,745 gradients, 28.7 GFLOPs\n\nTransferred 120/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 3256.6±340.3 MB/s, size: 2284.7 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/data_augmented/train/labels... 1687 images, 5 backgrounds, 0 corrupt: 100%|██████████| 1687/1687 [00:14<00:00, 116.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /kaggle/working/data_augmented/train/labels.cache\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 1912.0±1341.6 MB/s, size: 3015.9 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/data_augmented/val/labels... 154 images, 0 backgrounds, 0 corrupt: 100%|██████████| 154/154 [00:00<00:00, 341.97it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /kaggle/working/data_augmented/val/labels.cache\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/Final_Model_Custom/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.0008' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/Final_Model_Custom\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      1/100      7.31G      3.217      3.796      3.941         48        640:  95%|█████████▌| 101/106 [00:50<00:02,  1.98it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2727734539.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n--- Running FINAL POLISH training programmatically ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m model.train(\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_yaml_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/model.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m  \u001b[0;31m# attach optional HUB session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;31m# Update model and cfg after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mRANK\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworld_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/engine/trainer.py\u001b[0m in \u001b[0;36m_do_train\u001b[0;34m(self, world_size)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTQDM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_batch_start\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0;31m# Warmup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ultralytics/data/build.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;34m\"\"\"Create an iterator that yields indefinitely from the underlying iterator.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1410\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":39},{"cell_type":"code","source":"import ultralytics.nn.modules as ultralytics_modules\nimport ultralytics.nn.tasks as ultralytics_tasks\n\nprint(\"\\nDefining and patching custom modules...\")\n\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super(ChannelAttention, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n        self.fc = nn.Sequential(nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False), nn.ReLU(), nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False))\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        return self.sigmoid(avg_out + max_out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super(SpatialAttention, self).__init__()\n        padding = 3 if kernel_size == 7 else 1\n        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)\n        self.sigmoid = nn.Sigmoid()\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        return self.sigmoid(self.conv1(x))\n\nclass CBAM(nn.Module):\n    def __init__(self, c1, c2):\n        super().__init__()\n        self.channel_attention = ChannelAttention(c1)\n        self.spatial_attention = SpatialAttention()\n    def forward(self, x):\n        x = x * self.channel_attention(x)\n        return x * self.spatial_attention(x)\n\nclass PConv(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n        self.conv = nn.Conv2d(dim // 2, dim // 2, 3, padding=1, groups=dim // 2)\n        self.act = nn.ReLU(inplace=True)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        out_channel = x.shape[1] // 2\n        x[:, :out_channel, :, :] = self.conv(x[:, :out_channel, :, :])\n        return self.act(x)\n\nultralytics_modules.__dict__['CBAM'] = CBAM\nultralytics_modules.__dict__['PConv'] = PConv\nultralytics_tasks.__dict__['CBAM'] = CBAM\nultralytics_tasks.__dict__['PConv'] = PConv\n\ndef siou_loss(pred, target, eps=1e-7):\n    b1_x1, b1_y1, b1_x2, b1_y2 = pred.chunk(4, -1)\n    b2_x1, b2_y1, b2_x2, b2_y2 = target.chunk(4, -1)\n    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n    union = w1 * h1 + w2 * h2 - inter + eps\n    iou = inter / union\n    c_x, c_y = (b1_x1 + b1_x2) / 2, (b1_y1 + b1_y2) / 2\n    c_x_gt, c_y_gt = (b2_x1 + b2_x2) / 2, (b2_y1 + b2_y2) / 2\n    cw, ch = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1), torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)\n    sigma = torch.pow(c_x - c_x_gt, 2) + torch.pow(c_y - c_y_gt, 2)\n    sin_alpha_1 = torch.abs(c_y - c_y_gt) / torch.sqrt(sigma + eps)\n    sin_alpha_2 = torch.abs(c_x - c_x_gt) / torch.sqrt(sigma + eps)\n    threshold = math.pi / 4\n    angle_cost = torch.sin(2 * torch.arcsin(torch.sin(torch.abs(torch.asin(sin_alpha_1) - threshold))))\n    rho_x = (c_x_gt - c_x) / cw\n    rho_y = (c_y_gt - c_y) / ch\n    gamma = 2 - angle_cost\n    distance_cost = 1 - torch.exp(-gamma * (torch.pow(rho_x, 2) + torch.pow(rho_y, 2)))\n    omega_w = torch.abs(w1 - w2) / torch.max(w1, w2)\n    omega_h = torch.abs(h1 - h2) / torch.max(h1, h2)\n    shape_cost = torch.pow(1 - torch.exp(-1 * omega_w), 4) + torch.pow(1 - torch.exp(-1 * omega_h), 4)\n    siou_loss = 1 - iou + distance_cost + shape_cost\n    return siou_loss\nBboxLoss.iou = siou_loss\nprint(\"Custom modules and SIoU loss patched.\")\n\n\n# ===================================================================\n# STEP 4: CREATE CUSTOM MODEL YAML FILE\n# ===================================================================\nyolov8s_custom_yaml = \"\"\"\nnc: 3\ndepth_multiple: 0.33\nwidth_multiple: 0.50\n\nbackbone:\n  [[-1, 1, Conv, [32, 3, 2]],\n   [-1, 1, Conv, [64, 3, 2]],\n   [-1, 1, C2f, [64, 64, 1, True]],\n   [-1, 1, Conv, [128, 3, 2]],\n   [-1, 2, C2f, [128, 128, 2, True]],\n   \n   [-1, 1, CBAM, [128, 128]],\n   [-1, 1, PConv, [128]],\n\n   [-1, 1, Conv, [256, 3, 2]],\n   [-1, 2, C2f, [256, 256, 2, True]],\n   [-1, 1, Conv, [512, 3, 2]],\n   [-1, 1, C2f, [512, 512, 1, True]],\n   \n   [-1, 1, SPPF, [512, 512]]\n  ]\n\nhead:\n  [[-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 6], 1, Concat, [1]],\n   [-1, 1, C2f, [768, 256, 1]],\n   \n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n   [[-1, 4], 1, Concat, [1]],\n   [-1, 1, C2f, [384, 128, 1]],\n   \n   [-1, 1, nn.Dropout2d, [0.1]],\n   \n   [-1, 1, Conv, [128, 128, 3, 2]],\n   [[-1, 12], 1, Concat, [1]],\n   [-1, 1, C2f, [384, 256, 1]],\n   \n   [-1, 1, Conv, [256, 256, 3, 2]],\n   [[-1, 9], 1, Concat, [1]],\n   [-1, 1, C2f, [768, 512, 1]],\n   \n   # CRITICAL FIX: The Detect module arguments are now correctly formatted\n   [[15, 18, 21], 1, Detect, [3, [128, 256, 512]]]\n  ]\n\"\"\"\nwith open(yaml_path, 'w') as f:\n    f.write(yolov8s_custom_yaml)\nprint(\"Custom model YAML file created.\")\n\n# ===================================================================\n# STEP 5: CONFIGURE AND RUN TRAINING PROGRAMMATICALLY\n# ===================================================================\nwith open(data_yaml_path, 'w') as f:\n    yaml.dump({\n        'path': new_data_dir,\n        'train': 'train/images',\n        'val': 'val/images',\n        'test': 'test/images',\n        'nc': 3,\n        'names': ['FireExtinguisher', 'ToolBox', 'OxygenTank']\n    }, f)\nprint(\"\\nCreated data configuration file.\")\n\nprint(\"\\n--- Running FINAL POLISH training programmatically ---\")\nos.chdir(working_dir)\n\nmodel = YOLO(yaml_path)\nmodel.train(\n    data=data_yaml_path,\n    epochs=100,\n    imgsz=640,\n    batch=16,\n    mosaic=1.0,\n    lr0=0.0008,\n    cos_lr=True,\n    project='runs/detect',\n    name='Final_Model_Custom',\n    patience=25\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T00:23:04.231188Z","iopub.execute_input":"2025-08-02T00:23:04.231927Z","iopub.status.idle":"2025-08-02T00:23:08.593539Z","shell.execute_reply.started":"2025-08-02T00:23:04.231898Z","shell.execute_reply":"2025-08-02T00:23:08.592539Z"}},"outputs":[{"name":"stdout","text":"\nDefining and patching custom modules...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1016/477337044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDefining and patching custom modules...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mChannelAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_planes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mChannelAttention\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"],"ename":"NameError","evalue":"name 'nn' is not defined","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===================================================================\n# STEP 1: INSTALL DEPENDENCIES\n# ===================================================================\nprint(\"Installing dependencies...\")\n!pip install ultralytics -q\n!pip install pyyaml -q\n!pip install opencv-python -q\n!pip install timm -q\nprint(\"Installation complete!\")\n\n# ===================================================================\n# STEP 2: DEFINE AND PATCH CUSTOM MODULES AND LOSS FUNCTION\n# ===================================================================\nimport os\nimport shutil\nimport yaml\nimport cv2\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\nimport timm\nfrom ultralytics.nn import tasks\nfrom ultralytics.nn.modules import Conv, C2f, Concat\nfrom ultralytics.nn.modules.head import Detect\nfrom ultralytics.utils.metrics import bbox_iou\nfrom ultralytics.utils.ops import xywh2xyxy\nfrom ultralytics.utils.loss import BboxLoss\n\nprint(\"\\nDefining and patching custom modules...\")\n\n# 1. Register Detect in globals (fix KeyError)\nglobals()[\"Detect\"] = Detect\n\n# 2. Custom Swin Transformer Backbone\nclass TimmBackbone(torch.nn.Module):\n    def __init__(self, model_name='swin_small_patch4_window7_224', pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(\n            model_name,\n            features_only=True,\n            pretrained=pretrained,\n            out_indices=(1, 2, 3)\n        )\n\n        # Disable strict input size check (important!)\n        if hasattr(self.model, 'patch_embed') and hasattr(self.model.patch_embed, 'strict_img_size'):\n            self.model.patch_embed.strict_img_size = False\n\n        self.out_channels = self.model.feature_info.channels()\n        print(f\"Swin Backbone initialized. Output channels: {self.out_channels}\")\n\n    def forward(self, x):\n        return self.model(x)\n\n# Register the custom backbone\nfrom ultralytics.nn import tasks\ntasks.TimmBackbone = TimmBackbone\nglobals()[\"TimmBackbone\"] = TimmBackbone\n\n# 4. Define SIoU Loss (simple placeholder for patching demo)\ndef siou_loss(pred, target, eps=1e-7):\n    iou = bbox_iou(xywh2xyxy(pred), xywh2xyxy(target), xywh=False, CIoU=True)\n    return iou  # Placeholder: use your full SIoU logic\n\nBboxLoss.iou_function = siou_loss\nprint(\"Custom module 'TimmBackbone' and SIoU loss patched.\")\n\n# ===================================================================\n# STEP 3: CREATE CUSTOM MODEL YAML FILE\n# ===================================================================\nprint(\"\\nCreating custom model YAML...\")\n\nyolov8s_swin_yaml = \"\"\"\nnc: 3\nbackbone:\n  - [-1, 1, TimmBackbone, ['swin_small_patch4_window7_224']]\n\nhead:\n  - [-1, 1, Conv, [512, 1, 1]]\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 1], 1, Concat, [1]]\n  - [-1, 1, C2f, [896, 256, 1, False]]\n\n  - [-1, 1, Conv, [256, 1, 1]]\n  - [-1, 1, nn.Upsample, [None, 2, 'nearest']]\n  - [[-1, 0], 1, Concat, [1]]\n  - [-1, 1, C2f, [448, 128, 1, False]]\n\n  - [-1, 1, Conv, [128, 3, 2]]\n  - [[-1, 6], 1, Concat, [1]]\n  - [-1, 1, C2f, [384, 256, 1, False]]\n\n  - [-1, 1, Conv, [256, 3, 2]]\n  - [[-1, 3], 1, Concat, [1]]\n  - [-1, 1, C2f, [768, 512, 1, False]]\n\n  - [[8, 11, 14], 1, Detect, [nc]]\n\"\"\"\n\nworking_dir = '/kaggle/working/'\nyaml_path = os.path.join(working_dir, 'yolov8s-swin.yaml')\nwith open(yaml_path, 'w') as f:\n    f.write(yolov8s_swin_yaml)\nprint(\"Custom Swin Transformer model YAML file created.\")\n\n# ===================================================================\n# STEP 4: DATA PREPARATION (AUGMENTATION)\n# ===================================================================\nsource_data_dir = '/kaggle/input/falcon/HackByte_Dataset/data'\nnew_data_dir = os.path.join(working_dir, 'data_augmented')\n\nif not any(f.startswith('aug_occlusion_') for f in os.listdir(os.path.join(new_data_dir, 'train', 'images')) if os.path.exists(new_data_dir)):\n    print(\"Performing data augmentation...\")\n    if os.path.exists(new_data_dir): shutil.rmtree(new_data_dir)\n    shutil.copytree(source_data_dir, new_data_dir, dirs_exist_ok=True)\n    # Example augmentation logic can be added here\nelse:\n    print(\"Augmented data found. Skipping augmentation.\")\n\n# Create data config YAML\ndata_yaml_path = os.path.join(working_dir, 'data_config.yaml')\nwith open(data_yaml_path, 'w') as f:\n    yaml.dump({\n        'path': new_data_dir,\n        'train': 'train/images',\n        'val': 'val/images',\n        'test': 'test/images',\n        'nc': 3,\n        'names': ['FireExtinguisher', 'ToolBox', 'OxygenTank']\n    }, f)\n\n# ===================================================================\n# STEP 5: TRAIN THE MODEL\n# ===================================================================\nprint(\"\\nStarting model training...\")\nos.chdir(working_dir)\n\n# Load and train\nmodel.train(\n    data=data_yaml_path,\n    epochs=100,\n    imgsz=640,\n    batch=8,\n    project='runs/detect',\n    name='swin_yolov8s_siou',\n    patience=25\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T00:23:28.345826Z","iopub.execute_input":"2025-08-02T00:23:28.346641Z","iopub.status.idle":"2025-08-02T00:23:45.621282Z","shell.execute_reply.started":"2025-08-02T00:23:28.346610Z","shell.execute_reply":"2025-08-02T00:23:45.620393Z"}},"outputs":[{"name":"stdout","text":"Installing dependencies...\nInstallation complete!\n\nDefining and patching custom modules...\nCustom module 'TimmBackbone' and SIoU loss patched.\n\nCreating custom model YAML...\nCustom Swin Transformer model YAML file created.\nAugmented data found. Skipping augmentation.\n\nStarting model training...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_1016/1138105755.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;31m# Load and train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m model.train(\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_yaml_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"# ===================================================================\n# STEP 1: DEFINE AND PATCH CUSTOM MODULES AND LOSS FUNCTION\n# ===================================================================\nimport os\nimport shutil\nimport cv2\nimport numpy as np\nimport random\nimport yaml\nimport torch\nimport torch.nn as nn\nfrom ultralytics import YOLO\nimport timm\nfrom ultralytics.nn import tasks\nfrom ultralytics.nn.modules import Conv, C2f, Concat\nfrom ultralytics.utils.loss import BboxLoss\nfrom ultralytics.utils.ops import xywh2xyxy\nfrom ultralytics.nn.tasks import Detect\n\nprint(\"\\nDefining and patching custom modules for resuming...\")\n\n# 1. Custom Backbone Definition\nclass TimmBackbone(torch.nn.Module):\n    def __init__(self, model_name='swin_small_patch4_window7_224', pretrained=True):\n        super().__init__()\n        self.model = timm.create_model(model_name, features_only=True, pretrained=pretrained, out_indices=(1, 2, 3))\n        self.out_channels = self.model.feature_info.channels()\n        print(f\"Swin Backbone initialized. Output channels: {self.out_channels}\")\n\n    def forward(self, x):\n        features = self.model(x)\n        return features[1], features[2], features[3]\n\n# 2. Patch the custom module into the tasks dictionary\ntasks.TimmBackbone = TimmBackbone\n\n# 3. Define and Patch the SIoU Loss\nfrom ultralytics.utils.metrics import bbox_iou\ndef siou_loss(pred, target, eps=1e-7):\n    iou = bbox_iou(xywh2xyxy(pred), xywh2xyxy(target), xywh=False, CIoU=True)\n    return iou\nBboxLoss.iou_function = siou_loss\nprint(\"Custom module 'TimmBackbone' and SIoU loss patched.\")\n\n\n# ===================================================================\n# STEP 2: RESUME TRAINING\n# ===================================================================\nimport os\n\nprint(\"\\nResuming model training...\")\nworking_dir = '/kaggle/working/'\nnew_data_dir = os.path.join(working_dir, 'data_augmented')\ndata_yaml_path = os.path.join(working_dir, 'data_config.yaml')\ncheckpoint_path = os.path.join(working_dir, 'runs/detect/swin_yolov8s_siou/weights/last.pt')\n\nif not os.path.exists(checkpoint_path):\n    print(f\"Error: Checkpoint file not found at {checkpoint_path}\")\n    print(\"Please ensure your training run saved a 'last.pt' file in the correct directory.\")\nelse:\n    # Change the current directory to where the training happened\n    os.chdir(working_dir)\n\n    # Load the last checkpoint\n    model = YOLO(checkpoint_path)\n\n    # Resume training from the loaded checkpoint with the same parameters\n    model.train(\n        data=data_yaml_path,\n        epochs=100,\n        imgsz=640,\n        batch=8,\n        project='runs/detect',\n        name='swin_yolov8s_siou',\n        patience=25,\n        resume=True # <<< This is the key argument\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T23:38:25.832484Z","iopub.execute_input":"2025-08-01T23:38:25.832759Z","iopub.status.idle":"2025-08-01T23:47:48.601096Z","shell.execute_reply.started":"2025-08-01T23:38:25.832734Z","shell.execute_reply":"2025-08-01T23:47:48.600411Z"}},"outputs":[{"name":"stdout","text":"\nDefining and patching custom modules for resuming...\nCustom module 'TimmBackbone' and SIoU loss patched.\n\nResuming model training...\nUltralytics 8.3.172 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/kaggle/working/data_config.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=/kaggle/working/runs/detect/swin_yolov8s_siou/weights/last.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=swin_yolov8s_siou, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=25, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=/kaggle/working/runs/detect/swin_yolov8s_siou/weights/last.pt, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=runs/detect/swin_yolov8s_siou, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.0, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n 22        [15, 18, 21]  1   2117209  ultralytics.nn.modules.head.Detect           [3, [128, 256, 512]]          \nModel summary: 129 layers, 11,136,761 parameters, 11,136,745 gradients, 28.7 GFLOPs\n\nTransferred 355/355 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 4320.6±1088.8 MB/s, size: 2900.0 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/data_augmented/train/labels.cache... 846 images, 5 backgrounds, 0 corrupt: 100%|██████████| 846/846 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2081.1±1419.0 MB/s, size: 3015.9 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/data_augmented/val/labels.cache... 154 images, 0 backgrounds, 0 corrupt: 100%|██████████| 154/154 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/swin_yolov8s_siou/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\nResuming training /kaggle/working/runs/detect/swin_yolov8s_siou/weights/last.pt from epoch 84 to 100 total epochs\nImage sizes 640 train, 640 val\nUsing 2 dataloader workers\nLogging results to \u001b[1mruns/detect/swin_yolov8s_siou\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     84/100      7.71G     0.3905     0.3068     0.8682         11        640: 100%|██████████| 106/106 [00:28<00:00,  3.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:02<00:00,  3.47it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.994      0.912      0.952      0.888\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     85/100      2.27G     0.3764     0.2825     0.8638         23        640: 100%|██████████| 106/106 [00:28<00:00,  3.72it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.991      0.918       0.96      0.897\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     86/100      2.27G     0.3589     0.2707     0.8541         19        640: 100%|██████████| 106/106 [00:28<00:00,  3.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.20it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.991      0.918      0.958      0.894\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     87/100      2.27G     0.3625     0.2791     0.8543         14        640: 100%|██████████| 106/106 [00:28<00:00,  3.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.989      0.932      0.956       0.89\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     88/100      2.27G     0.3397     0.2583     0.8494         13        640: 100%|██████████| 106/106 [00:28<00:00,  3.69it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.17it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.993      0.914      0.955      0.892\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     89/100      2.27G     0.3482     0.2554     0.8488         21        640: 100%|██████████| 106/106 [00:28<00:00,  3.73it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.04it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.993      0.916      0.955      0.889\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     90/100      2.27G      0.344     0.2653     0.8574         13        640: 100%|██████████| 106/106 [00:28<00:00,  3.71it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.23it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.984      0.922      0.953       0.89\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"Closing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     91/100      2.27G     0.2621     0.2029     0.7833          8        640: 100%|██████████| 106/106 [00:29<00:00,  3.59it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.993      0.903      0.947      0.886\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     92/100      2.31G     0.2612     0.1957     0.7775          8        640: 100%|██████████| 106/106 [00:27<00:00,  3.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.16it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206       0.99      0.903      0.944       0.89\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     93/100      2.31G     0.2638     0.1972     0.7885          6        640: 100%|██████████| 106/106 [00:28<00:00,  3.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.15it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.986      0.909      0.948      0.891\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     94/100      2.31G     0.2608      0.193     0.7816         10        640: 100%|██████████| 106/106 [00:28<00:00,  3.78it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.16it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.986      0.921      0.953      0.892\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     95/100      2.31G     0.2607     0.1926     0.7818          8        640: 100%|██████████| 106/106 [00:28<00:00,  3.77it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.07it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.986      0.918      0.954      0.892\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     96/100      2.31G     0.2459     0.1849     0.7747          7        640: 100%|██████████| 106/106 [00:28<00:00,  3.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.29it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.984      0.913      0.949       0.89\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     97/100      2.34G     0.2569     0.1915     0.7864          7        640: 100%|██████████| 106/106 [00:28<00:00,  3.76it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.18it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.986      0.918      0.952      0.894\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     98/100      2.34G     0.2531     0.1902     0.7832          8        640: 100%|██████████| 106/106 [00:28<00:00,  3.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.12it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.985      0.919      0.948      0.889\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     99/100      2.34G     0.2507     0.1834     0.7824          8        640: 100%|██████████| 106/106 [00:28<00:00,  3.74it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.21it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.985      0.919      0.948      0.893\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"    100/100       2.4G     0.2509     0.1883     0.7879          7        640: 100%|██████████| 106/106 [00:27<00:00,  3.79it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  3.12it/s]","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.985      0.919      0.949      0.892\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"name":"stdout","text":"\n17 epochs completed in 0.152 hours.\nOptimizer stripped from runs/detect/swin_yolov8s_siou/weights/last.pt, 22.5MB\nOptimizer stripped from runs/detect/swin_yolov8s_siou/weights/best.pt, 22.5MB\n\nValidating runs/detect/swin_yolov8s_siou/weights/best.pt...\nUltralytics 8.3.172 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 11,126,745 parameters, 0 gradients, 28.4 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 10/10 [00:03<00:00,  2.96it/s]\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n","output_type":"stream"},{"name":"stdout","text":"                   all        154        206      0.991      0.918       0.96      0.897\n      FireExtinguisher         67         67      0.999      0.955      0.978      0.923\n               ToolBox         60         60      0.991        0.9      0.947      0.918\n            OxygenTank         79         79      0.984      0.899      0.953      0.849\nSpeed: 0.2ms preprocess, 3.3ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to \u001b[1mruns/detect/swin_yolov8s_siou\u001b[0m\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# ===================================================================\n# STEP 1: IMPORT DEPENDENCIES AND SET UP\n# ===================================================================\nimport os\nfrom ultralytics import YOLO\nimport yaml\n\nprint(\"Setting up paths for validation...\")\nworking_dir = '/kaggle/working/'\nresults_dir = os.path.join(working_dir, 'runs/detect/train2')\nbest_model_path = os.path.join(results_dir, 'weights/best.pt')\nyolo_params_path = os.path.join(working_dir, 'yolo_params.yaml')\n\n# --- Check if the model and data config exist before proceeding ---\nif not os.path.exists(best_model_path):\n    print(f\"Error: Best model not found at {best_model_path}\")\n    print(\"Please ensure your training run completed successfully and the path is correct.\")\nelif not os.path.exists(yolo_params_path):\n    print(f\"Error: yolo_params.yaml not found at {yolo_params_path}\")\n    print(\"Please ensure the file exists in your working directory.\")\nelse:\n    print(f\"Found trained model at: {best_model_path}\")\n    print(f\"Found data config at: {yolo_params_path}\")\n    \n    # --- Update yolo_params.yaml to point to the test set ---\n    print(\"\\nUpdating yolo_params.yaml to point to the test dataset for validation.\")\n    with open(yolo_params_path, 'r') as f:\n        yolo_config = yaml.safe_load(f)\n\n    # CRITICAL FIX: The val path must be relative to the 'path' key\n    # which is already set to '/kaggle/working/data_augmented'.\n    yolo_config['val'] = 'test/images'\n    \n    with open(yolo_params_path, 'w') as f:\n        yaml.dump(yolo_config, f, sort_keys=False)\n\n    # ===================================================================\n    # STEP 2: RUN THE MODEL ON THE TEST SET\n    # ===================================================================\n    print(\"\\nStarting validation on the test set...\")\n\n    # Change the current directory to where the model was trained\n    os.chdir(working_dir)\n\n    # Load the best model from the training run\n    model = YOLO(best_model_path)\n\n    # Validate the model. The results will be printed and saved to the results directory.\n    results = model.val(data=yolo_params_path)\n\n   \nprint(\"\\n--- Final Performance Metrics on Test Set ---\")\n# CORRECTED ATTRIBUTES: maps50 -> map50, maps -> map\nprint(f\"Overall mAP50: {results.box.map50}\")\nprint(f\"Overall mAP50-95: {results.box.map}\")\nprint(f\"Class-specific P, R, mAP50, mAP50-95: {results.box.mp}, {results.box.mr}, {results.box.map50}, {results.box.map}\")\nprint(\"------------------------------------------\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T23:57:07.056971Z","iopub.execute_input":"2025-08-01T23:57:07.057526Z","iopub.status.idle":"2025-08-01T23:57:22.233963Z","shell.execute_reply.started":"2025-08-01T23:57:07.057498Z","shell.execute_reply":"2025-08-01T23:57:22.233086Z"}},"outputs":[{"name":"stdout","text":"Setting up paths for validation...\nFound trained model at: /kaggle/working/runs/detect/train2/weights/best.pt\nFound data config at: /kaggle/working/yolo_params.yaml\n\nUpdating yolo_params.yaml to point to the test dataset for validation.\n\nStarting validation on the test set...\nUltralytics 8.3.172 🚀 Python-3.11.13 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\nModel summary (fused): 72 layers, 11,126,745 parameters, 0 gradients, 28.4 GFLOPs\n\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 2978.1±1225.1 MB/s, size: 1813.1 KB)\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/data_augmented/test/labels.cache... 400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 400/400 [00:00<?, ?it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:11<00:00,  2.18it/s]\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n/usr/local/lib/python3.11/dist-packages/matplotlib/colors.py:721: RuntimeWarning: invalid value encountered in less\n  xa[xa < 0] = -1\n","output_type":"stream"},{"name":"stdout","text":"                   all        400        560      0.955      0.809      0.889      0.808\n      FireExtinguisher        183        183      0.962      0.847      0.897      0.789\n               ToolBox        193        193      0.953      0.813      0.903      0.835\n            OxygenTank        184        184       0.95      0.766      0.868      0.802\nSpeed: 0.4ms preprocess, 4.9ms inference, 0.0ms loss, 1.2ms postprocess per image\nResults saved to \u001b[1mruns/detect/val5\u001b[0m\n\n--- Final Performance Metrics on Test Set ---\nOverall mAP50: 0.8889286585375306\nOverall mAP50-95: 0.8084343011137606\nClass-specific P, R, mAP50, mAP50-95: 0.9549166153102506, 0.8089234619786287, 0.8889286585375306, 0.8084343011137606\n------------------------------------------\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}